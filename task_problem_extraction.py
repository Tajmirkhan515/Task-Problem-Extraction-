# -*- coding: utf-8 -*-
"""Task problem Extraction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YAJrhdpjbeYZeCxFl7i0_CETf4-LPqgu
"""

!pip install transformers
import nltk
nltk.download('punkt')

!pip install sumy # summarization library

from nltk.tokenize import sent_tokenize # Generate a sentences list from the whole paragraph

from transformers import pipeline               
classifier = pipeline("text-classification")  # pre trained model for sentence classification, positive negative

# for summarization, using extractive method
import sumy
from sumy.summarizers.lsa import LsaSummarizer
#Plain text parsers since we are parsing through text
from sumy.parsers.plaintext import PlaintextParser
#for tokenization
from sumy.nlp.tokenizers import Tokenizer

# extract indroduction of a paper
sentences=sent_tokenize("""
 A recent trend to measure progress towards machine reading is to test a system's ability to answer questions about a document it has to comprehend. Towards this end, several large-scale datasets of cloze-style questions over a context document have been introduced recently, which allow the training of supervised machine learning systems. Such datasets can be easily constructed automatically and the unambiguous nature of their queries provides an objective benchmark to measure a system's performance at text comprehension.
Deep learning models have been shown to outperform traditional shallow approaches on text comprehension tasks. The success of many recent models can be attributed primarily to two factors: (1) Multi-hop architectures, allow a model to scan the document and the question iteratively for multiple passes.
(2) Attention mechanisms, borrowed from the machine translation literature, allow the model to focus on appropriate subparts of the context document. Intuitively, the multi-hop architecture allows the reader to incrementally refine token representations, and the attention mechanism re-weights different parts in the document according to their relevance to the query.
The effectiveness of multi-hop reasoning and attentions have been explored orthogonally so far in the literature. In this paper, we focus on combining both in a complementary manner, by designing a novel attention mechanism which gates the evolving token representations across hops. More specifically, unlike existing models where the query attention is applied either token-wise or sentence-wise to allow weighted aggregation, the Gated-Attention (GA) module proposed in this work allows the query to directly interact with each dimension of the token embeddings at the semantic-level, and is applied layer-wise as information filters during the multi-hop representation learning process. Such a fine-grained attention enables our model to learn conditional token representations w.r.t. the given question, leading to accurate answer selections.
We show in our experiments that the proposed GA reader, despite its relative simplicity, consis-tently improves over a variety of strong baselines on three benchmark datasets . Our key contribution, the GA module, provides a significant improvement for large datasets. Qualitatively, visualization of the attentions at intermediate layers of the GA reader shows that in each layer the GA reader attends to distinct salient aspects of the query which help in determining the answer.

""")

nearToProblemIde=""
for sentence in sentences:
  if sentence.__contains__('['):    # remove those sentences which contains refrences
    continue    
  output_predict=classifier(sentence)
  output_predict=output_predict[0]
  if output_predict.get('label')=='POSITIVE':
    continue
  #print(output_predict.get('label'))
  #print(output_predict.get('score'))
  #print(sentence)
  nearToProblemIde=nearToProblemIde+str(output_predict.get('score'))+":"+output_predict.get('label')+""+sentence
##  [{'label': 'POSITIVE', 'score': 0.99}

nearToProblemIde=nearToProblemIde.replace("\n"," ")
nearToProblemIde=nearToProblemIde.replace(".",". \n")
print(nearToProblemIde)

#Generate summary of each paper, 
summarizer_2 = LsaSummarizer()
parser = PlaintextParser.from_string(nearToProblemIde,Tokenizer("english"))
summary_2 =summarizer_2(parser.document,7)
print(len(summary_2))
for sentence in summary_2:
    print(sentence)

!pip install simpletransformers

!pip install git+https://github.com/huggingface/transformers

import logging
import pandas as pd
import sklearn
from simpletransformers.classification import ClassificationModel, ClassificationArgs

from simpletransformers.classification import (
    ClassificationArgs,
    ClassificationModel,
)

logging.basicConfig(level=logging.WARNING)
transformers_logger = logging.getLogger("transformers")
transformers_logger.setLevel(logging.WARNING)

df = pd.read_csv('/content/sample_data/all_sent.csv')
df = df.drop(columns=['BIO_2', 'labels']).rename(
    columns={'bi_labels': 'labels'})
df['title'] = df['main_heading'] + ': ' + df['heading']
df.loc[((df['main_heading'] == df['heading']) | (
    pd.isnull(df['heading']))), 'title'] = df['main_heading']
df['title'] = df['title'].fillna('')

model_args = ClassificationArgs()

model_args.normalize_ofs = True
model_args.overwrite_output_dir = True
model_args.reprocess_input_data = True
model_args.use_multiprocessing = False
model_args.manual_seed = 1
model_args.fp16 = False
model_args.do_lower_case = True

# Create a TransformerModel
# Create a ClassificationModel
#model = ClassificationModel(
#    "roberta","roberta-base", args=model_args,
#)
model = ClassificationModel(
    "roberta", "roberta-base", use_cuda=False
)

result, model_outputs, wrong_predictions = model.eval_model(
    df, F1_score=sklearn.metrics.f1_score)

predictions = model_outputs.argmax(axis=1)
# select the sentences that are predicted positive, to be the input for subtask 2
mask = df['mask'].values
print("LLLLLLLLLLLLL: mask: ",mask)
# sentences in the 'related work' or 'conclusion' sections are forced to be negative
predictions = predictions * mask
pos = df[predictions == 1]
pos.to_csv('pos_sent.csv', index=False)